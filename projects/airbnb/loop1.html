---
layout: single
classes: wide
title: Loop 1 & ETL
toc: true
toc_label: "On this page"
toc_sticky: true
author_profile: true
---

<!-- Mermaid JS -->
<script type="module">
  import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs";
  mermaid.initialize({ startOnLoad: true, theme: "mc", look: "handDrawn" });
</script>

<main>

  <section id="nyc-etl-overview">

    <h2 id="Overview">ETL Overview</h2>

    <p>
      <a href="https://github.com/jonnylgreenwood/airbnb-snowflake-dbt/tree/main/airbnb" target="_blank"><img
          src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/github.svg" alt="GitHub" width="20"
          height="20" style="vertical-align:middle; margin-right:4px;">Code ↗</a> |
      <a href="{{ site.baseurl }}/projects/airbnb/target/" target="_blank"><img
          src="{{ site.baseurl }}/projects/airbnb/images/dbt Logos/dbt-bit-standalone.png" alt="dbt Docs" width="20"
          height="20" style="vertical-align:middle; margin-right:4px;">dbt Docs ↗</a> |
      <a href="https://app.snowflake.com/hukihlk/kvb89980/#/workspaces/ws/USER%24/PUBLIC/airbnb-snowflake-dbt"
        target="_blank"><img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons@16.2.0/icons/snowflake.svg"
          alt="Snowflake" width="20" height="20" style="vertical-align:middle; margin-right:4px;">Snowflake ↗</a> |
      <a href="https://github.com/jonnylgreenwood/Business-Insights/issues/84" target="_blank"><img
          src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/github.svg" alt="GitHub" width="20"
          height="20" style="vertical-align:middle; margin-right:4px;">Project Management ↗</a>
    </p>

    <p>
      This project ingests Airbnb data from multiple online sources, each differing in structure, format, and data
      quality, and updated on an ongoing basis. This variability introduces complexity that requires a <b>
        structured,
        auditable ETL approach
      </b> rather than ad-hoc transformation.

      To manage this, I built a <b>cloud-based ETL pipeline</b> using Snowflake and dbt, organised into three layers:
    </p>

    <ul>
      <li><strong>L0 [Bronze]</strong> – Ingest high-volume source files to preserve data fidelity and enable auditing.
      </li>
      <li><strong>L1 [Silver]</strong> – Clean, reshape, and unify data for relational consistency.</li>
      <li><strong>L2 [Gold]</strong> – Aggregate and prepare data for analysis, modelling, and reporting.</li>
    </ul>

    <p>The diagram below illustrates how data flows through these layers into analytics-ready outputs.</p>

    {% include diagrams/airbnb-etl.html %}

    <p>
      This ETL setup is a critical component of Framework Loop 1, establishing the foundation required to answer
      early-stage business questions. Subsequent loops may ingest additional data sources and repeat parts of this
      process as analytical needs evolve.
    </p>

    <p>
    <h2 id="file-storage">File Storage</h2>
    </p>
    <p>

      Before transformation begins, data storage and project structure are defined to ensure traceability and
      reproducibility.
    </p>
    <p>

      A Snowflake database is created, and dbt sources, models, and folder conventions are used to explicitly define
      schemas and data lineage. This ensures every table can be traced back to its originating file and transformation
      logic.
    </p>

    <p>
      Example schema structure:
      <code>airbnb.l0.example_data</code>,
      <code>l2.fact_reviews</code>.
    </p>
    <p>Centralising storage in Snowflake simplifies downstream access while maintaining clear separation between raw,
      cleansed, and curated data.</p>




    <h2 id="l0">L0 [Bronze]</h2>
    <p>
      L0 captures data exactly as received, without modification.
    </p>


    <h3 id="l0-Data-Acquisition">Data Acquisition</h3>

    <p>
      Python is used to scrape csv.gz files directly from the <a href="https://insideairbnb.com/get-the-data/">Inside
        Airbnb open-data portal</a>. Files are downloaded programmatically across multiple regions and placed into a
      <code>data/raw_data</code> directory.
    </p>
    <p>
      Python is then used to convert the csv.gz files to .parquet before being
      uploaded to Snowflake.
    </p>

    <p>Python was chosen for this step because:</p>
    <ul>
      <li>Error handling and logging are more explicit than Snowflake CSV ingestion</li>
      <li>Schema inspection is easier prior to load</li>
      <li>The process is repeatable as new source files are added</li>
    </ul>

    <h3 id="L0-Load">L0 Load</h3>

    <p>Each file is loaded into Snowflake as a separate raw table, preserving original schemas and values. No cleaning
      or transformation occurs at this stage.</p>

    <p>This layer exists purely to support auditing, replayability, and data confidence.</p>



    <h2 id="l1">L1 [Silver]</h2>

    <p>
      The Silver layer focuses on cleaning, standardising, and validating the raw data.
      It’s where inconsistencies are corrected, schemas are aligned, and initial business rules are applied to make the
      data
      structurally consistent across sources.




    </p>
    <h3 id="l1-Data-Profiling">Data Profiling</h3>
    <p>
      Before transformation, source-level sampling was performed to understand:

    <ul>
      <li>Which columns exist and how they differ by source</li>
      <li>Column meaning and data types</li>
      <li>Common quality issues to address proactively</li>
    </ul>
    </p>
    <h3 id="L1-Load">L1 Load</h3>
    <p>
      During the transition from L0 → L1:

    <ul>
      <li>Field names are standardised</li>
      <li>Data types are explicitly cast</li>
      <li>Schemas are aligned across regions</li>
      <li>Records are grouped into consistent fact-style tables</li>
      <li>At this point, the data becomes relationally usable, but still avoids aggregation or opinionated business
        logic.</li>
    </ul>
    </p>

    <h2 id="l2">L2 &#91;Gold&#93;</h2>
    <p>The Gold layer contains data in its final, analytics-ready form.</p>
    <p>For Framework Loop 1, this layer focuses on producing descriptive, decision-support tables rather than predictive
      models.</p>
    <p>Example outputs:</p>
    <ul>
      <li><code>l2.NEIGHBOURHOOD_PROPERTIES</code></li>
      <li><code>l2.NEIGHBOURHOOD_MARKET_SNAPSHOT</code></li>
    </ul>
    These tables were designed specifically to support a rapid Excel-based feasibility model.

<h2 id="Analytics-Reporting-Modelling">Analytics / Reporting / Modelling</h2>
<h3 id="excel">Excel Model</h3>
<p>The Excel model was built to support rapid feasibility analysis, allowing users to explore occupancy, rates, and listing density by city and neighbourhood.</p>
<iframe width="1100" height="500" frameborder="0" scrolling="no" src="https://1drv.ms/x/c/c3ba3e2f3280a745/IQSRFsGHmRuIRKP1CgMUhCqBAd_mLZ9oZYO-ea3uP1CRFWA?AllowTyping=True&ActiveCell='Cost Analysis'!B2&wdHideGridlines=True&wdHideHeaders=True&wdInConfigurator=True&wdInConfigurator=True"></iframe>

<h3 id="snowflake-examples">Snowflake Examples</h3>

<p>
      Example Snowflake dimension table:

      <a href="{{ site.baseurl }}/projects/airbnb/images/snowflake_2.png" target="_blank">
        <img src="{{ site.baseurl }}/projects/airbnb/images/snowflake_2.png" alt="Preview"
          style="width:100%; height:auto; border-radius:8px;">
      </a>
    </p>

  </section>